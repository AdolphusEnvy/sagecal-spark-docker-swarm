These instructions were tested on Linux system. It should also work on different systems. If you are aving problems, please let us know by creating an issue.


# Prerequisites
 - docker (version 18.0 or higher)
 - docker-compose (1.20 or higher)

# Usage
## 1-Swarm manager
To initialize the swarm setup run:
```console
foo@bar:~$ docker swarm init

Swarm initialized: current node (hzywvwt5zygzctmrv4k1hrjfp) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-3hlnrriihgjm4ajgmk8drpe5my7kzprtjmgh2qrh8akw64jy98-6vrrb008zo22sk76lr4c7q2qb 10.14.0.164:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
```

Your system will be the Swarm manager node. Take a note of the **token** generated by this command. You will use this token if you want to add other nodes to the Swarm cluster.


## 2-Create an overlay and cluster networks
```console
foo@bar:~$ docker network create --opt encrypted --driver overlay --attachable dirac_distributed
```


## 3-Pull the Docker images
Sometimes the Docker images are not pulled automatically. It is better that you download them before deploying the services.

```console
foo@bar:~$ docker-compose pull 
```

You can also pull the images manually:
```
foo@bar:~$ docker pull fdiblen/spark-master-dirac && docker pull fdiblen/spark-worker-dirac && docker pull portainer/portainer && docker pull minio/minio  
```

## 4-Create services

### Minio
To deploy Minio services run:
```console
foo@bar:~$ docker stack deploy --compose-file=minio-swarm.yaml minio
```
Keys used in the setup are shown below:

access key : nlesc

secret key: dirac

**Important Note:** Minio secret keys and access keys are stored in `minio-swarm.yaml` and should be changed in production environment.


### Apache Spark
To deploy Spark services run:
```console
foo@bar:~$ docker stack deploy --resolve-image always --compose-file=spark-swarm.yml spark
```

Depending on your system or Docker version you may get an error. In this case try the following command:
```console
foo@bar:~$ docker stack deploy --compose-file=spark-swarm.yml spark
```

### Container Management UI (Portainer)

```console
foo@bar:~$ docker stack deploy --compose-file=vis.yml vis
```
**Important Note:** Authentication of the UI is disabled. This should be change in production environment.


## 5-Web interfaces
You can now access to the web interface of the created services:

Spark interface [http://0.0.0.0:8080/](http://0.0.0.0:8080)

Minio interface [http://0.0.0.0:9001](http://0.0.0.0:9001)

Container manager [http://0.0.0.0:3000/](http://0.0.0.0:3000)

**Note:** Secret key and access key of Minio is explained in [Minio section](####Minio).


# Scaling the services
Number of available Spark workers can easily be scaled up/down. In order to have 4 Spark worker nodes run:
```console
foo@bar:~$ docker service scale spark_worker=4
```

# Info about the services

List the services:
```console
foo@bar:~$ docker stack services spark
```
List the tasks:
```console
foo@bar:~$ docker stack ps spark
```
```console
foo@bar:~$ docker network inspect docker_gwbridge | egrep 'Name|IPv4>
```

```
docker stack services spark
docker service inspect --pretty  spark_master
docker service inspect --pretty  spark_worker
docker service ps spark_master
docker service ps spark_worker
```

# Stopping the services
```console
foo@bar:~$ docker stack rm spark
foo@bar:~$ docker stack rm minio
```

# Clean up containers
```
docker stop $(docker ps -a -q) && docker rm $(docker ps -a -q)
```

# Debugging

You can check the logs
```console
foo@bar:~$ docker logs -f container_name
```

The container name can be found by
```console
foo@bar:~$ docker stack ps spark
```

