# Spark and Hadoop in Docker Swarm

# Prerequisites
 - docker (version 18.0 or higher)
 - docker-compose (1.20 or higher)

# Usage
## 1 Swarm manager
To initialize the swarm setup run:
```
docker swarm init
```
Your system will be the Swarm manager node. Take a note of the **token** generated by this command. You will use this token if you want to add other nodes to the Swarm cluster.

## 2 Create an overlay and cluster networks
```
docker network create --ingress --driver overlay ingress
docker network create -d overlay --attachable spark-net
```
## 3 Create services
To deploy Spark and Hadoop services run:
```
docker stack deploy --resolve-image always -c spark-swarm.yml spark
```

# Scaling the services
Number of available Spark workers can easily be scaled up/down. In order to have 4 Spark worker nodes run:
```
docker service scale spark_worker=4
```

# Info about the services
List the services:
```
docker stack services spark
```
List the tasks:
```
docker stack ps spark
```
```
docker network inspect docker_gwbridge | egrep 'Name|IPv4>
```

# Stopping the services
```
docker stack rm spark
```

# Debugging

You can check the logs
```
docker logs -f container_name
```

The container name can be found by
```
docker stack ps spark
```
